name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
      - develop

# Si hay otro workflow corriendo en la misma rama, lo cancelamos.
# Así evitamos gastar minutos de CI innecesarios.

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  REGISTRY: ghcr.io
  IMAGE_OWNER: ${{ github.repository_owner }}

jobs:

  # 1: Lint y pruebas (corren en paralelo para cada servicio)

  lint-and-test:
    name: Lint & Test — ${{ matrix.service }}                                          # 2 En esta parte corro las pruebas unitarias y de linting para cada servicio, usando una matriz de servicios. Esto me permite validar que cada servicio es correcto antes de construir las imágenes.
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false          # Si un servicio falla, el otro sigue ejecutándose
      matrix:
        service: [api-gateway, user-service]
    defaults:
      run:
        working-directory: apps/${{ matrix.service }}                                  # 2.2 En esta parte gracias a este codigo se ejecutan las pruebas unitarias, abajo se define matrix.service, entonces el working directory se va a ir cambiando dependiendo del servicio que se esté testeando, así puedo correr las pruebas unitarias de cada servicio de forma independiente.

    steps:
      - name: Clonar repositorio
        uses: actions/checkout@v4

      - name: Configurar Node.js 20
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: apps/${{ matrix.service }}/package-lock.json

      - name: Instalar dependencies
        run: npm ci

      - name: Ejecutar linting
        run: npm run lint --if-present

      - name: Ejecutar tests con coverage                                             # 2.3 En esta parte corro las pruebas unitarias que se ejecutan en api-gateway y user-service, y genero un reporte de coverage en formato lcov para cada servicio. Esto me permite tener métricas de cobertura específicas por servicio.
        run: npm test -- --ci --coverage --coverageReporters=lcov

      - name: Subir reporte de coverage
        uses: actions/upload-artifact@v4
        
        # Aunque falle algo, intenta subir el reporte

        if: always() 
        with:
          name: coverage-${{ matrix.service }}
          path: apps/${{ matrix.service }}/coverage/
          retention-days: 7

  # 2: Construcción de imágenes en Docker + push al registry + escaneo de seguridad

  build-and-push:
    name: Build & Push — ${{ matrix.service }}
    runs-on: ubuntu-latest
    needs: lint-and-test
    
    # Solo construye imágenes cuando es un push real (no en PR)
    
    if: github.event_name == 'push'
    permissions:
      contents: read
      packages: write
      security-events: write    # Necesario para subir resultados SARIF
    strategy:
      fail-fast: false
      matrix:
        service: [api-gateway, user-service]                                        # 1.2 y 2.2 En esta parte defino la matriz de servicios, por cada servicio se va a ejecutar este job, y se van a construir las imagenes correspondientes a cada servicio. 

    outputs:
      
      # Guarda el digest de la imagen para poder usar referencias inmutables

      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Clonar repositorio
        uses: actions/checkout@v4

      - name: Configurar Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login en GitHub Container Registry                                    # 3.1 Empujo la imagen al GitHUb Container Registry (GHCR) con el login-action oficial de Docker. Uso el token automático de GitHub para autenticación, que tiene permisos limitados y es seguro para este propósito.
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}                                     # 3.2 Token automático de GitHub

      # Estrategia de tags:
      # - sha-xxxxxxx: versión inmutable para despliegues
      # - develop: última construcción de develop
      # - latest: última construcción de main

      - name: Generar metadata de Docker                                            # 3.3 Nombre de la imagen, tags y labels usando metadata-action. Esto me permite tener tags consistentes y también incluir información útil en los labels de la imagen (como el commit SHA, repo URL, etc).
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ matrix.service }}
          tags: |
            type=sha,prefix=sha-,format=short
            type=ref,event=branch
            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}
          labels: |
            org.opencontainers.image.title=${{ matrix.service }}
            org.opencontainers.image.source=${{ github.server_url }}/${{ github.repository }}
            org.opencontainers.image.revision=${{ github.sha }}

      # Construcción de Multi-platform y push 

      - name: Build y push de la imagen Docker        
        id: build
        uses: docker/build-push-action@v5                                           # 1.1 En esta parte exacta construyo las imagenes, se lee el Dockerfile, se construye la imagen, se etiqueta. También se genera SBOM y se firma la imagen.
        with:
          context: apps/${{ matrix.service }}
          target: final           
          push: true                                                                # 1.3 y 3.5 Push automático al registry
          tags: ${{ steps.meta.outputs.tags }}                                      # 3.4 Uso los tags generados por metadata-action
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          provenance: true        
          sbom: true             

      # Escaneo básico de vulnerabilidades con Trivy.
      # No se rompe el pipeline si encuentra algo, solo reporta.

      - name: Escanear imagen con Trivy
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ matrix.service }}:sha-${{ github.sha }}
          format: 'sarif'
          output: trivy-results-${{ matrix.service }}.sarif
          severity: 'CRITICAL,HIGH'
          exit-code: '0'          

      - name: Subir resultados de seguridad a GitHub
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: trivy-results-${{ matrix.service }}.sarif
          category: trivy-${{ matrix.service }}

  # 3: Desplegar a entorno DEV (cuando se hace push a develop)

  deploy-dev:                                                                         # 4.1 Aca inicio el deployment a Kubernetes en DEV, abajo muestro donde ocurre exactamente el deployment
    name: Deploy → DEV
    runs-on: ubuntu-latest
    needs: build-and-push                                                             # 4.4 En la siguiente linea se ejecuta el despliegue en DEV es decir en el if de abajo, se indica que solo se despliegue a DEV cuando el evento sea un push a la rama develop, esto me permite que cada vez que haga un push a develop se actualice automáticamente el entorno de desarrollo con la nueva versión de la aplicación. Esto es ideal para validar cambios rápidamente en un entorno similar a producción antes de hacer un release oficial.
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'  
    environment:
      name: dev
      url: https://dev.devops-challenge.example.com
    env:
      DEPLOY_ENV: dev
      K8S_NAMESPACE: devops-challenge-dev
      IMAGE_TAG: sha-${{ github.sha }}

    steps:
      - name: Clonar repositorio
        uses: actions/checkout@v4

      - name: Configurar credenciales AWS
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Actualizar kubeconfig (EKS)                                              # 4.2 Aca antes del deployment sonfiguro el acceso a Kubernetes usando aws cli, esto me permite que los siguientes comandos de kubectl puedan comunicarse con el cluster EKS de AWS.
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME_DEV }} \
            --region ${{ secrets.AWS_REGION }}

      - name: Instalar kustomize
        uses: imranismail/setup-kustomize@v2

      - name: Actualizar tags de imagen en overlay DEV
        working-directory: k8s/overlays/dev
        run: |
          kustomize edit set image \
            ghcr.io/${{ env.IMAGE_OWNER }}/api-gateway=ghcr.io/${{ env.IMAGE_OWNER }}/api-gateway:${{ env.IMAGE_TAG }} \
            ghcr.io/${{ env.IMAGE_OWNER }}/user-service=ghcr.io/${{ env.IMAGE_OWNER }}/user-service:${{ env.IMAGE_TAG }}

      - name: Despliegue en entorno DEV                                                # 4.3 Aca exactamente ocurre el deployment real de DEV ya que aca Kubernetes aplica los manifiestos y actualiza los deployments con las nuevas imágenes. Luego espera a que los pods estén listos antes de continuar.
        run: |
          kubectl apply -k k8s/overlays/dev
          kubectl rollout status deployment/api-gateway -n ${{ env.K8S_NAMESPACE }} --timeout=300s
          kubectl rollout status deployment/user-service -n ${{ env.K8S_NAMESPACE }} --timeout=300s

      - name: Smoke test básico en DEV
        run: |
          # Wait for pods to be fully ready
          sleep 10
          # Port-forward and test health
          kubectl port-forward svc/api-gateway 3000:3000 -n ${{ env.K8S_NAMESPACE }} &
          sleep 5
          curl --fail --retry 3 http://localhost:3000/health || exit 1
          echo "Smoke test passed"

      - name: Notificar éxito en Slack
        if: success()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "*DEV Deploy Successful*\nService: devops-challenge\nSHA: `${{ github.sha }}`\nTriggered by: ${{ github.actor }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

      - name: Notify on failure
        if: failure()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "*DEV Deploy FAILED*\nService: devops-challenge\nSHA: `${{ github.sha }}`\nTriggered by: ${{ github.actor }}\nSee: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

  
  # 4: Deploy a PRODUCCIÓN (push a main, con aprobación manual)
  
  deploy-prod:
    name: Deploy → PROD                                                             # 5.1 Aca se inicia el deployment de Kubernetes en producción, abajo se indica en donde exactamente ocurre el deployment, es similar a dev pero con algunos pasos adicionales como obtener secretos de AWS Secrets Manager, y también incluye un paso de rollback automático en caso de fallo.
    runs-on: ubuntu-latest
    needs: build-and-push                                                           # 5.4 En la siguiente linea se ejecuta el despliegue en producción solo cuando el evento sea un push a la rama main, esto me permite que cada vez que haga un push a main se actualice automáticamente el entorno de producción con la nueva versión de la aplicación. Es importante destacar que en un escenario real probablemente querría agregar una aprobación manual antes de desplegar a producción, pero para este ejercicio lo dejo automático para demostrar el flujo completo.
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production              # Ambiente de GitHub con revisiones requeridas
      url: https://devops-challenge.example.com
    env:
      DEPLOY_ENV: prod
      K8S_NAMESPACE: devops-challenge-prod
      IMAGE_TAG: sha-${{ github.sha }}

    steps:
      - name: Clonar repositorio
        uses: actions/checkout@v4

      - name: Configurar credenciales AWS (Producción)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Actualizar kubeconfig PROD                                             # 5.2 Aca configuro el acceso a Kubernetes para producción, usando las credenciales de AWS específicas para producción. Esto me permite que los comandos de kubectl posteriores puedan comunicarse con el cluster EKS de producción.
        run: |
          aws eks update-kubeconfig \                                       
            --name ${{ secrets.EKS_CLUSTER_NAME_PROD }} \                     
            --region ${{ secrets.AWS_REGION }}

      - name: Instalar kustomize
        uses: imranismail/setup-kustomize@v2

      - name: Obtener secretos desde AWS Secrets Manager
        run: |
          # Pull secrets and write to secrets.env for kustomize secretGenerator
          REDIS_PASSWORD=$(aws secretsmanager get-secret-value \
            --secret-id devops-challenge/prod/redis-password \
            --query SecretString --output text)
          echo "redis-password=${REDIS_PASSWORD}" > k8s/overlays/prod/secrets.env

      - name: Actualizar imágenes en overlay PROD
        working-directory: k8s/overlays/prod
        run: |
          kustomize edit set image \
            ghcr.io/${{ env.IMAGE_OWNER }}/api-gateway=ghcr.io/${{ env.IMAGE_OWNER }}/api-gateway:${{ env.IMAGE_TAG }} \
            ghcr.io/${{ env.IMAGE_OWNER }}/user-service=ghcr.io/${{ env.IMAGE_OWNER }}/user-service:${{ env.IMAGE_TAG }}
          # Also update APP_VERSION with git SHA for traceability
          kustomize edit add configmap app-config --from-literal=APP_VERSION=${{ env.IMAGE_TAG }}

      - name: Despliegue en PRODUCCIÓN                                                # 5.3 Aquí ocurre el despliegue real a producción, se aplican los manifiestos de Kubernetes y se espera a que los deployments estén actualizados y los pods estén listos. Se usan tiempos de espera más largos para producción.
        run: |
          kubectl apply -k k8s/overlays/prod
          kubectl rollout status deployment/api-gateway -n ${{ env.K8S_NAMESPACE }} --timeout=600s
          kubectl rollout status deployment/user-service -n ${{ env.K8S_NAMESPACE }} --timeout=600s

      - name: Verificación básica en producción
        run: |
          kubectl port-forward svc/api-gateway 3000:3000 -n ${{ env.K8S_NAMESPACE }} &
          sleep 5
          curl --fail --retry 5 --retry-delay 3 http://localhost:3000/health || exit 1
          curl --fail --retry 5 --retry-delay 3 http://localhost:3000/health/ready || exit 1
          echo "Production health checks passed"

      - name: Limpiar archivo temporal de secretos
        if: always()
        run: rm -f k8s/overlays/prod/secrets.env

      - name: Notificación de exito en Slack
        if: success()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "*PROD Deploy Successful*\nService: devops-challenge\nSHA: `${{ github.sha }}`\nTriggered by: ${{ github.actor }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK

      - name: Notificar fallo — auto rollback
        if: failure()
        run: |
          echo "Prod deploy failed — rolling back"
          kubectl rollout undo deployment/api-gateway -n ${{ env.K8S_NAMESPACE }} || true
          kubectl rollout undo deployment/user-service -n ${{ env.K8S_NAMESPACE }} || true

      - name: Notificación de fallo en Slack
        if: failure()
        uses: slackapi/slack-github-action@v1.26.0
        with:
          payload: |
            {
              "text": "*PROD Deploy FAILED — Rollback initiated*\nService: devops-challenge\nSHA: `${{ github.sha }}`\nTriggered by: ${{ github.actor }}\nSee: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
